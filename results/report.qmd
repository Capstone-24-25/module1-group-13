---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "List names here"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
```

## Abstract: Nikhil

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset: Leena

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers: Leena

Tasks 1-2

### Methodlogical variations: Nikhil/Sanchit/Shirley

a\) repeat the analysis on training partition

```{r}
set.seed(1)

#partition data (80%)
partitions <- biomarker_clean %>%
  initial_split(prop = 0.8)
train_data <- training(partitions)

```

Multiple Testing Method

```{r}
## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train_data %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
New_proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

```

the Multiple Testing Method in inclass-analysis gives us 10 proteins:

"DERM" "RELT"

"Calcineurin" "C1QR1"

"MRC2" "IgD"

"CXCL16, soluble" "PTN"

"FSTL1" "Cadherin-5"

The modified method gives us 10 different proteins since the data we used is different: "C1QR1" "TGF-b R III" "IgD" "CXCL16, soluble" "FSTL1" "MMP-2" "gp130, soluble" "ROR1" \] "MRC2" "RELT"

the majority of the proteins are the same

Random Forest

```{r}
## RANDOM FOREST
##################

# store predictors and response separately
predictors <- train_data %>%
  select(-c(group, ados))

response <- train_data %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
New_proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)


```

For old random forest method, it returns the confusion matrix as below:

```         
    ASD TD class.error 
ASD  48 28   0.3684211 
TD   17 61   0.2179487
```

and top 10 proteins are:

```         
DERM"        "IgD"         "TGF-b R III"  [4] "MAPK14"      "FSTL1"       "RELT"         [7] "eIF-4H"      "M2-PK"       "SOST"        [10] "ALCAM"     
```

The modified random forest method returns the confusion matrix as below:

```         
    ASD TD class.error 
ASD  33 25   0.4310345 
TD   17 48   0.2615385
```

and the top 10 proteins it returns are:

```         
[1] "IgD"            "MMP-2"          "ERBB1"           [4] "MAPK14"         "gp130, soluble" "FSTL1"           [7] "CHL1"           "ALCAM"          "Notch 1"        [10] "ROR1"
```

These two methods returns different top 10 proteins\
the old method performs better inclassyfying both ASD and TD groups

```{r}
## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(New_proteins_s1, New_proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')

```

the new one:

+-------------+------------+-----------+
| .metric     | .estimator | .estimate |
|             |            |           |
| \<chr\>     | \<chr\>    | \<dbl\>   |
+:============+:===========+==========:+
| sensitivity | binary     | 0.8750000 |
+-------------+------------+-----------+
| specificity | binary     | 0.7333333 |
+-------------+------------+-----------+
| accuracy    | binary     | 0.8064516 |
+-------------+------------+-----------+
| roc_auc     | binary     | 0.8583333 |
+-------------+------------+-----------+

Compared to the old method,

The new method is not good at identifying true positives.

The new method also has more false positive so more Type I errors

The new method has worse accuracy so performs worse in classification of both positives and negatives

The new method ahs worse ability to distinguish between classes.

b\) choose a larger number of top predictive proteins

b.1) Multiple Testing Method

I choose 15 predictive proteins.\
the old method returns:

```         
 [1] "DERM"            "RELT"             [3] "Calcineurin"     "C1QR1"            [5] "MRC2"            "IgD"              [7] "CXCL16, soluble" "PTN"              [9] "FSTL1"           "Cadherin-5"
```

```{r}
# function to compute tests
# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins

proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 15) %>%
  pull(protein)
```

The new methods returns:

```         
 [1] "DERM"            "RELT"             [3] "Calcineurin"     "C1QR1"            [5] "MRC2"            "IgD"              [7] "CXCL16, soluble" "PTN"              [9] "FSTL1"           "Cadherin-5"      [11] "MAPK2"           "TGF-b R III"     [13] "DAF"             "MMP-2"           [15] "gp130, soluble" 
```

b.2)

```{r}
## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
New_proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 15) %>%
  pull(protein)

```

It returns:

```         
 [1] "DERM"        "IgD"         "TGF-b R III"  [4] "MAPK14"      "FSTL1"       "RELT"         [7] "eIF-4H"      "M2-PK"       "SOST"        [10] "ALCAM"       "MAPK2"       "CK-MB"       [13] "RET"         "Calcineurin" "TSP4"   
```

b.3)

```{r}
## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(New_proteins_s1, New_proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

based on the metrics value, the old method has a slightly higher sensitivity so it is more accurately identifies true positives.

The new method is less effective at avoiding false positives which means more Type I errors

the old method demonstrates a higher overall accuracy, so it's better at classifying both positive and negative cases.

The old method has better overall discriminatory power, so its better at distinguishing between classes.

### Improved classifier: Nikhil/Sanchit/Shirley

```{r}
proteins_simpler <- rf_out$importance %>%
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  filter(protein != "class") %>%         
  slice_max(MeanDecreaseGini, n = 5) %>%  
  pull(protein)

biomarker_simpler <- biomarker_clean %>%
  select(group, any_of(proteins_simpler)) %>% 
  mutate(class = (group == 'ASD')) %>% 
  select(-group)

set.seed(101422)
biomarker_split_simpler <- biomarker_simpler %>%
  initial_split(prop = 0.8)

fit_simpler <- glm(class ~ ., 
                   data = training(biomarker_split_simpler), 
                   family = 'binomial')

class_metrics <- metric_set(sensitivity, specificity, accuracy, roc_auc)

test_results <- testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%   
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) 

test_results <- testing(biomarker_split_simpler) %>%
  add_predictions(fit_simpler, type = 'response') %>%  
  mutate(
    est = as.factor(pred > 0.5),  
    tr_c = as.factor(class)       
  )

test_results %>%
  class_metrics(
    estimate = est,
    truth = tr_c,
    pred,                     
    event_level = 'second'
  )
```

```{r}

```
