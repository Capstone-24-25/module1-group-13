---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "List names here"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
```

## Abstract: Nikhil

This report presents a statistical analysis to identify blood biomarkers for Autism Spectrum Disorder (ASD) in young boys, using a dataset of protein levels from 154 participants. The analysis encompasses three main methods---multiple testing, correlation with severity (ADOS scores), and random forest---to identify and prioritize proteins that significantly differentiate between ASD and typically developing (TD) participants. Through preprocessing, including log transformations and outlier handling, the dataset is normalized for further statistical evaluation. Top predictive proteins are identified and evaluated for their accuracy in ASD classification using logistic regression and LASSO feature selection. The report details the methods' performance metrics and provides insights into the proteins most indicative of ASD, proposing a panel of biomarkers with optimized accuracy and significance.

## Dataset Description:

The dataset involves the analysis of 1,125 different proteins to conclude whether or not certain proteins were blood biomarkers for Autism Spectrum Disorder in boys. Data was obtained from a total of 154 boys with an average age of about 5.65. 76 of the participants were clinically diagnosed with Autism Spectrum Disorder (ASD) and 78 were confirmed to be typically-developing (TD) boys. Blood samples were taken from all participants and analyzed using the SOMAScan platform 1.3k to measure levels of binding to specific proteins.

Obtained data regarding protein abundance was normalized through both log10 and z-transformations. Outliers outside of the range \[-3,3\] after the z-transformation were dropped. Proteins that were determined to predict ASD were identified through random forest method, t-tests, and correlation methods. The metric of significance was area under the curve.

## Summary of published analysis
The study utilized three methods, multiple testing, correlation with severity, and random forest. Multiple testing involves hypothesis testing and t-tests to determine which proteins result in significantly different levels of serum. In the case of the study, the top 10 proteins with the greatest significance were utilized in prediction. Correlation with severity involves finding the strongest correlations between different proteins and ADOS. In the case of the study, they measured protein correlation to ADOS total scores. Variable importance involves a measurement for which predictors have most influence in a random forest. In the case of the study, they trained a model to predict protein importance in ASD in comparison to TD. They were then able to rank these different proteins by importance. With the top-performing proteins from each approach identified, the panel identified the 'core' proteins by determining their presence in each test result.

The five 'core' proteins common across each of the three methods were: mitogen-activated protein kinase 14 (MAPK14), immunoglobulin D (IgD), dermatopontin (DERM), ephrin type-B receptor 2 (EPHB2), and soluble urokinase-type plasminogen activator receptor (suPAR). To optimize AUC score, the following four proteins were added to the five 'core' proteins: receptor tyrosine kinase-like orphan receptor 1 \[ROR1\], platelet receptor Gl24 \[GI24\], eukaryotic translation initiation factor 4H \[elF-4H\], and arylsulfatase B \[ARSB\]. The 'optimal' nine proteins resulted in a AUC score of 0.860±0.064, where sensitivity (true positive rate) was 0.833±0.118 and specificity (true negative rate) was 0.846±0.118.

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers: Leena

Tasks 1-2

1.  What do you imagine is the reason for log-transforming the protein levels in \`biomarker-raw.csv\`? (Hint: look at the distribution of raw values for a sample of proteins.)

Potential reasons for log-transforming the protein levels in \`biomarker-raw.csv\` include normalizing skewed data by reducing the impact of outliers. Observe the skew and distribution of the raw biomarker data below.
```{r}
raw_data <- read.csv("../data/biomarker-raw.csv")

sample_data <- raw_data %>%
  select(sample(colnames(raw_data), 25)) %>%
  pivot_longer(everything(), names_to = "protein", values_to = "level")

ggplot(sample_data, aes(x = as.numeric(level))) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Distribution of Raw Protein Levels",
       x = "Protein Level",
       y = "Frequency")
```

\
Our plot confirms that the raw data is significantly right skewed, with majority of protein levels on or at zero. We can assume that log-transforming the protein levels was a necessary step to ensure that the data could be further analyzed by methods that assume a normal distribution, like the regression that occurs at the latter half of the experiment.

1.  Temporarily remove the outlier trimming from preprocessing and do some exploratory analysis of outlying values. Are there specific *subjects* (not values) that seem to be outliers? If so, are outliers more frequent in one group or the other? (Hint: consider tabluating the number of outlying values per subject.)

### Methodlogical variations: Nikhil/Sanchit/Shirley

a\) repeat the analysis on training partition

```{r}
set.seed(1)

#partition data (80%)
partitions <- biomarker_clean %>%
  initial_split(prop = 0.8)
train_data <- training(partitions)

```

Multiple Testing Method

```{r}
## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train_data %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
New_proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

```

the Multiple Testing Method in inclass-analysis gives us 10 proteins:

"DERM" "RELT"

"Calcineurin" "C1QR1"

"MRC2" "IgD"

"CXCL16, soluble" "PTN"

"FSTL1" "Cadherin-5"

The modified method gives us 10 different proteins since the data we used is different: "C1QR1" "TGF-b R III" "IgD" "CXCL16, soluble" "FSTL1" "MMP-2" "gp130, soluble" "ROR1" \] "MRC2" "RELT"

the majority of the proteins are the same

Random Forest

```{r}
## RANDOM FOREST
##################

# store predictors and response separately
predictors <- train_data %>%
  select(-c(group, ados))

response <- train_data %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
New_proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)


```

For old random forest method, it returns the confusion matrix as below:

```         
    ASD TD class.error 
ASD  48 28   0.3684211 
TD   17 61   0.2179487
```

and top 10 proteins are:

```         
DERM"        "IgD"         "TGF-b R III"  [4] "MAPK14"      "FSTL1"       "RELT"         [7] "eIF-4H"      "M2-PK"       "SOST"        [10] "ALCAM"     
```

The modified random forest method returns the confusion matrix as below:

```         
    ASD TD class.error 
ASD  33 25   0.4310345 
TD   17 48   0.2615385
```

and the top 10 proteins it returns are:

```         
[1] "IgD"            "MMP-2"          "ERBB1"           [4] "MAPK14"         "gp130, soluble" "FSTL1"           [7] "CHL1"           "ALCAM"          "Notch 1"        [10] "ROR1"
```

These two methods returns different top 10 proteins\
the old method performs better inclassyfying both ASD and TD groups

```{r}
## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(New_proteins_s1, New_proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')

```

the new one:

+-------------+------------+-----------+
| .metric     | .estimator | .estimate |
|             |            |           |
| \<chr\>     | \<chr\>    | \<dbl\>   |
+:============+:===========+==========:+
| sensitivity | binary     | 0.8750000 |
+-------------+------------+-----------+
| specificity | binary     | 0.7333333 |
+-------------+------------+-----------+
| accuracy    | binary     | 0.8064516 |
+-------------+------------+-----------+
| roc_auc     | binary     | 0.8583333 |
+-------------+------------+-----------+

Compared to the old method,

The new method is not good at identifying true positives.

The new method also has more false positive so more Type I errors

The new method has worse accuracy so performs worse in classification of both positives and negatives

The new method ahs worse ability to distinguish between classes.

b\) choose a larger number of top predictive proteins

b.1) Multiple Testing Method

I choose 15 predictive proteins.\
the old method returns:

```         
 [1] "DERM"            "RELT"             [3] "Calcineurin"     "C1QR1"            [5] "MRC2"            "IgD"              [7] "CXCL16, soluble" "PTN"              [9] "FSTL1"           "Cadherin-5"
```

```{r}
# function to compute tests
# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins

proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 15) %>%
  pull(protein)
```

The new methods returns:

```         
 [1] "DERM"            "RELT"             [3] "Calcineurin"     "C1QR1"            [5] "MRC2"            "IgD"              [7] "CXCL16, soluble" "PTN"              [9] "FSTL1"           "Cadherin-5"      [11] "MAPK2"           "TGF-b R III"     [13] "DAF"             "MMP-2"           [15] "gp130, soluble" 
```

b.2)

```{r}
## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
New_proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 15) %>%
  pull(protein)

```

It returns:

```         
 [1] "DERM"        "IgD"         "TGF-b R III"  [4] "MAPK14"      "FSTL1"       "RELT"         [7] "eIF-4H"      "M2-PK"       "SOST"        [10] "ALCAM"       "MAPK2"       "CK-MB"       [13] "RET"         "Calcineurin" "TSP4"   
```

b.3)

```{r}
## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(New_proteins_s1, New_proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

based on the metrics value, the old method has a slightly higher sensitivity so it is more accurately identifies true positives.

The new method is less effective at avoiding false positives which means more Type I errors

the old method demonstrates a higher overall accuracy, so it's better at classifying both positive and negative cases.

The old method has better overall discriminatory power, so its better at distinguishing between classes.

### Improved classifier: Nikhil/Sanchit/Shirley

Q3. c)
```{r}
# Assume `New_proteins_s1` and `New_proteins_s2` are vectors containing top proteins from different selection methods
all_proteins <- c(New_proteins_s1, New_proteins_s2) # Combine top proteins from multiple methods

# Set fuzzy threshold: include proteins appearing in at least 70% of the methods
threshold <- 0.7 *2  # Update `2` based on the number of selection methods used
fuzzy_intersection_proteins <- names(protein_counts[protein_counts >= threshold])
fuzzy_metrics <- evaluate_model(fuzzy_intersection_proteins, response, predictors)

# Using hard intersection proteins for evaluation
hard_intersection_proteins <- intersect(New_proteins_s1, New_proteins_s2)
hard_metrics <- evaluate_model(hard_intersection_proteins, response, predictors)

# Print proteins from fuzzy intersection
print(fuzzy_intersection_proteins)
print(hard_intersection_proteins)
```
# Use `fuzzy_intersection_proteins` for modeling instead of `proteins_sstar`

```{r}
library(randomForest)
library(pROC) 

# evaluation function
evaluate_model <- function(selected_proteins, label_data, feature_data) {
  # Subset data by selected proteins
  model_data <- feature_data[, selected_proteins, drop = FALSE]
  
  # Train Random Forest
  rf_model <- randomForest(x = model_data, y = label_data, ntree = 1000, importance = TRUE)
  rf_predictions <- predict(rf_model, model_data, type = "prob")[,2]
  rf_auroc <- roc(label_data, rf_predictions)$auc
  
  # Train Logistic Regression
  logistic_model <- glm(label_data ~ ., data = model_data, family = "binomial")
  logistic_predictions <- predict(logistic_model, model_data, type = "response")
  logistic_auroc <- roc(label_data, logistic_predictions)$auc
  
  # Return AUROC values for comparison
  return(list(rf_auroc = rf_auroc, logistic_auroc = logistic_auroc))
}

# Display comparison results
comparison_df <- data.frame(
  Method = c("Hard Intersection", "Fuzzy Intersection"),
  RF_AUROC = c(hard_metrics$rf_auroc, fuzzy_metrics$rf_auroc),
  Logistic_AUROC = c(hard_metrics$logistic_auroc, fuzzy_metrics$logistic_auroc)
)
print(comparison_df)
```

```{r}
proteins_simpler <- rf_out$importance %>%
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  filter(protein != "class") %>%         
  slice_max(MeanDecreaseGini, n = 5) %>%  
  pull(protein)

biomarker_simpler <- biomarker_clean %>%
  select(group, any_of(proteins_simpler)) %>% 
  mutate(class = (group == 'ASD')) %>% 
  select(-group)

set.seed(101422)
biomarker_split_simpler <- biomarker_simpler %>%
  initial_split(prop = 0.8)

fit_simpler <- glm(class ~ ., 
                   data = training(biomarker_split_simpler), 
                   family = 'binomial')

class_metrics <- metric_set(sensitivity, specificity, accuracy, roc_auc)

test_results <- testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%   
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) 

test_results <- testing(biomarker_split_simpler) %>%
  add_predictions(fit_simpler, type = 'response') %>%  
  mutate(
    est = as.factor(pred > 0.5),  
    tr_c = as.factor(class)       
  )

test_results %>%
  class_metrics(
    estimate = est,
    truth = tr_c,
    pred,                     
    event_level = 'second'
  )
```

```{r}
library(glmnet)
library(rsample)

biomarker_clean <- biomarker_clean %>%
  mutate(class = ifelse(group == "ASD", 1, 0))

y <- biomarker_clean$class
table(y)

# Exclude non-numeric columns from biomarker_clean for model.matrix
biomarker_clean_numeric <- biomarker_clean %>%
  select(-group, -ados)  # Remove non-numeric or unwanted columns

biomarker_clean_numeric <- biomarker_clean_numeric %>%
  drop_na()

# Convert to model matrix
x <- as.matrix(biomarker_clean_numeric)
y <- biomarker_clean %>%
  filter(complete.cases(biomarker_clean_numeric)) %>%
  pull(class)

# Fit LASSO model with cross-validation to find optimal lambda
set.seed(101422)
lasso_model <- cv.glmnet(x, y, family = "binomial", alpha = 1)

# Get features with non-zero coefficients at optimal lambda
lasso_features <- rownames(coef(lasso_model, s = "lambda.min"))[-1]  # Skip intercept

# Filter biomarker_clean to only include selected LASSO features
biomarker_lasso <- biomarker_clean %>%
  select(group, any_of(lasso_features)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# Split data into training and test sets
biomarker_split_lasso <- initial_split(biomarker_lasso, prop = 0.8)
train_data <- training(biomarker_split_lasso)
test_data <- testing(biomarker_split_lasso)

train_data_reduced <- train_data %>%
  select(class, any_of(lasso_features[1:40]))  # Use top LASSO features

# Refit the logistic regression model
fit_lasso <- glm(class ~ ., data = train_data_reduced, family = "binomial")

# Define classification metrics explicitly
# Define classification metrics, excluding roc_auc for binary estimate
class_metrics <- metric_set(accuracy, sensitivity, specificity)

test_data <- test_data %>%
  mutate(
    class = as.factor(class),  # Convert class to factor
    pred = predict(fit_lasso, newdata = test_data, type = "response"),  # Probability
    est = as.factor(pred > 0.5)  # Binary classification
  )

# Evaluate the model on test data
test_results_lasso <- test_data %>%
  mutate(
    pred = predict(fit_lasso, newdata = test_data, type = "response"),  # Probability
    est = as.factor(pred > 0.5)  # Binary classification
  ) %>%
  class_metrics(truth = class, estimate = est)

# Combine the results
final_results <- bind_rows(test_results_lasso)

print(final_results)
```
